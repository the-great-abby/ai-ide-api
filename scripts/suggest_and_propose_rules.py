import os
import subprocess
import json
import requests
import sys

def get_default_url(port, path):
    if os.environ.get("RUNNING_IN_DOCKER") == "1":
        host = "host.docker.internal"
    else:
        host = "localhost"
    return f"http://{host}:{port}{path}"

OLLAMA_URL = os.environ.get(
    "OLLAMA_URL",
    get_default_url(11434, "/api/generate")
)
RULE_API_URL = os.environ.get(
    "RULE_API_URL",
    get_default_url(9103, "/propose-rule-change")
)
MODEL = os.environ.get("OLLAMA_MODEL", "llama3")


def run_static_checker(target="."):
    """Run scripts/suggest_rules.py and return its JSON output as a Python object."""
    result = subprocess.run([
        sys.executable, "scripts/suggest_rules.py", target
    ], capture_output=True, text=True)
    if result.returncode != 0:
        print("[ERROR] suggest_rules.py failed:", result.stderr)
        sys.exit(1)
    try:
        suggestions = json.loads(result.stdout)
    except Exception as e:
        print("[ERROR] Failed to parse suggest_rules.py output as JSON:", e)
        sys.exit(1)
    return suggestions


def build_llm_prompt(suggestions):
    """Build a prompt for Ollama from the static checker suggestions."""
    prompt = (
        "You are an expert code reviewer and rule author. "
        "Given the following static code analysis findings, generate clear, actionable, and well-documented rule proposals. "
        "For each unique rule_type, provide: a rule name, a description, enforcement steps, and a code example. "
        "Here are the findings (in JSON):\n" + json.dumps(suggestions, indent=2)
    )
    return prompt


def call_ollama(prompt):
    payload = {
        "model": MODEL,
        "prompt": prompt,
        "stream": False
    }
    resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()
    # Ollama returns {'response': '...'}
    return data.get("response", "")


def parse_llm_output(llm_output):
    """Very basic parser: expects LLM to output a JSON list of rule proposals."""
    try:
        # Try to extract JSON block if present
        start = llm_output.find("[")
        end = llm_output.rfind("]") + 1
        if start != -1 and end != -1:
            return json.loads(llm_output[start:end])
        # Fallback: try to parse whole output
        return json.loads(llm_output)
    except Exception as e:
        print("[ERROR] Failed to parse LLM output as JSON:", e)
        print("Raw output:\n", llm_output)
        return []


def submit_proposals(proposals):
    success = 0
    for proposal in proposals:
        try:
            resp = requests.post(RULE_API_URL, json=proposal, timeout=30)
            if resp.status_code == 200:
                print(f"[OK] Submitted proposal: {proposal.get('rule_type', proposal.get('rule_name', 'unknown'))}")
                success += 1
            else:
                print(f"[FAIL] Proposal submission failed: {resp.status_code} {resp.text}")
        except Exception as e:
            print(f"[ERROR] Exception submitting proposal: {e}")
    print(f"Submitted {success}/{len(proposals)} proposals successfully.")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Suggest and propose rules using static checkers and Ollama.")
    parser.add_argument("target", nargs="?", default=".", help="File or directory to scan")
    parser.add_argument("--dry-run", action="store_true", help="Only print proposals, do not submit")
    args = parser.parse_args()

    print("[1/4] Running static checkers...")
    suggestions = run_static_checker(args.target)
    if not suggestions:
        print("No suggestions found.")
        return
    print(f"Found {len(suggestions)} suggestions.\n")

    print("[2/4] Building LLM prompt and calling Ollama...")
    prompt = build_llm_prompt(suggestions)
    llm_output = call_ollama(prompt)

    print("[3/4] Parsing LLM output...")
    proposals = parse_llm_output(llm_output)
    if not proposals:
        print("No proposals generated by LLM.")
        return
    print(f"LLM generated {len(proposals)} proposals.\n")

    if args.dry_run:
        print(json.dumps(proposals, indent=2))
        return

    print("[4/4] Submitting proposals to API...")
    submit_proposals(proposals)

if __name__ == "__main__":
    main() 