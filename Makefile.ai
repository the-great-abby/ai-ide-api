PORT ?= 9103
ADMIN_FRONTEND_PORT ?= 3000
API_PORT ?= 9103
MEMORYDB_FILE ?= not_provided
RULESDB_FILE ?= not_provided
BACKUP_FILE ?= not_provided

# TODO: Implement a script for interactive Alembic head-checking and prompting for merges.
# This will help users resolve migration graph conflicts more safely and with better UX.

# Add this near the top with other variable definitions
# OLLAMA_MODEL ?= llama3.1:70b-instruct-q5_K_M
OLLAMA_MODEL ?= llama3.1:8b-instruct-q6_K

ai-up:
	docker-compose up -d api frontend
	$(MAKE) -f Makefile.ai ai-api-wait

ai-down:
	docker-compose down

api-up:
	docker compose up -d api db-test

ai-test:
	docker-compose run --rm test pytest tests/ --disable-warnings --tb=short

ai-test-json:
	docker-compose run --rm test pytest --json-report --json-report-file=pytest-report.json tests/

ai-test-one:
	docker-compose run --rm test pytest $(TEST) --disable-warnings --tb=short

ai-export-rules:
	python scripts/export_approved_rules.py

ai-lint-rules:
	python scripts/lint_rules.py

ai-env:
	curl -s http://localhost:$(PORT)/env || echo '{"error": "API not running"}'

ai-status:
	docker-compose ps

ai-propose-rule:
	curl -s -X POST http://localhost:$(PORT)/propose-rule-change \
	  -H "Content-Type: application/json" \
	  -d @$(RULE_FILE)

ai-list-pending:
	curl -s http://localhost:$(PORT)/pending-rule-changes

ai-approve-rule:
	curl -s -X POST http://localhost:$(PORT)/approve-rule-change/$(PROPOSAL_ID)

ai-reject-rule:
	curl -s -X POST http://localhost:$(PORT)/reject-rule-change/$(PROPOSAL_ID)

ai-list-rules:
	curl -s http://localhost:$(PORT)/rules

ai-list-rules-mdc:
	curl -s http://localhost:$(PORT)/rules-mdc

ai-review-code-files:
	curl -s -X POST http://localhost:$(PORT)/review-code-files \
	  -F "files=@$(FILE)"

ai-review-code-snippet:
	curl -s -X POST http://localhost:$(PORT)/review-code-snippet \
	  -H "Content-Type: application/json" \
	  -d '{"filename": "$(FILENAME)", "code": "$(CODE)"}'

ai-scan-db:
	echo '--- Proposals ---'; sqlite3 rules.db 'SELECT * FROM proposals;'; \
	echo '\n--- Rules ---'; sqlite3 rules.db 'SELECT * FROM rules;'; \
	echo '\n--- Feedback ---'; sqlite3 rules.db 'SELECT * FROM feedback;'

ai-approve-all-pending:
	python scripts/approve_all_pending.py

ai-db-migrate:
	docker compose exec api alembic upgrade head

ai-db-revision:
	docker compose exec api alembic revision --autogenerate -m "$(MSG)"

ai-db-autorevision:
	docker compose exec api alembic revision --autogenerate -m "$(MSG)"

# Propose the Makefile.ai-required rule to the API (uses /propose-rule-change and $(PORT))
ai-propose-makefile-rule:
	curl -s -X POST http://localhost:$(PORT)/propose-rule-change \
	  -H "Content-Type: application/json" \
	  -d '{ \
	    "rule_type": "$(RULE_TYPE)", \
	    "description": "$(DESCRIPTION)", \
	    "diff": "$(DIFF)", \
	    "submitted_by": "$(SUBMITTED_BY)", \
	    "categories": [$(CATEGORIES)], \
	    "tags": [$(TAGS)], \
	    "project": "$(PROJECT)", \
	    "applies_to": [$(APPLIES_TO)], \
	    "applies_to_rationale": "$(APPLIES_TO_RATIONALE)", \
	    "examples": "$(EXAMPLES)", \
	    "reason_for_change": "$(REASON_FOR_CHANGE)", \
	    "references": "$(REFERENCES)", \
	    "current_rule": "$(CURRENT_RULE)" \
	  }'

# Open or fetch the FastAPI docs endpoint
ai-docs:
	curl -s http://localhost:$(PORT)/docs || echo 'Could not fetch docs. Is the API running?'

# Show logs for all main containers (api, db-test, frontend)
logs:
	docker compose logs --tail=100 api db-test frontend || docker-compose logs --tail=100 api db-test frontend

# Show logs for just the API container
logs-api:
	docker compose logs --tail=100 api || docker-compose logs --tail=100 api

# Show logs for just the database container
logs-db:
	docker compose logs --tail=100 db-test || docker-compose logs --tail=100 db-test

# Show logs for just the frontend container
logs-frontend:
	docker compose logs --tail=100 frontend || docker-compose logs --tail=100 frontend

# Open the FastAPI docs in the default browser (macOS)
open-docs:
	open http://localhost:$(PORT)/docs

open-admin:
	open http://localhost:$(ADMIN_FRONTEND_PORT)

ai-build:
	docker-compose build

# Unified preflight check for VPNs and Zscaler
preflight-check:
	@if ifconfig | grep -E 'utun|tun0|ppp0' > /dev/null || \
	   ps aux | grep -E 'openvpn|nordvpn|expressvpn|protonvpn' | grep -v grep > /dev/null; then \
		echo '⚠️  VPN detected! Docker builds may fail, but continuing...'; \
	fi; \
	if ps aux | grep -i zscaler | grep -v grep > /dev/null || systemextensionsctl list | grep -i zscaler > /dev/null; then \
		echo '⚠️  Zscaler detected! Docker builds may fail.'; \
		echo 'Please turn off Zscaler Internet Security (ZIA) or pause it if possible.'; \
		echo 'If you cannot fully quit Zscaler, try pausing web/internet security in the Zscaler app.'; \
		echo 'Press Enter to continue (or Ctrl+C to abort)...'; \
		read; \
	else \
		echo 'No Zscaler detected. Proceeding...'; \
	fi

# Make ai-rebuild-all depend on preflight-check
ai-rebuild-all: preflight-check
	$(MAKE) -f Makefile.ai ai-db-backup-data-only
	docker-compose down
	docker system prune -af
	docker-compose build

ai-import-backup:
	docker compose exec api python import_from_backup.py

#
# ai-enable-fdw: Enable the postgres_fdw extension in both memorydb and rulesdb databases.
# Usage:
#   make -f Makefile.ai ai-enable-fdw
#
ai-enable-fdw:
	docker compose exec db-test psql -U postgres -d memorydb -c "CREATE EXTENSION IF NOT EXISTS postgres_fdw;"
	docker compose exec db-test psql -U postgres -d rulesdb -c "CREATE EXTENSION IF NOT EXISTS postgres_fdw;"

#
# ai-smart-merge-backup: Merge a backup SQL file into the live database using the smart_merge_backup.py script.
# Usage:
#   make -f Makefile.ai ai-smart-merge-backup BACKUP=/path/to/backup.sql
#
ai-smart-merge-backup: ai-enable-fdw ai-misc-install
	python3 smart_merge_backup.py $(BACKUP)

#
# ai-smart-merge-all-backups: Merge all backup SQL files in the backups directory using the smart_merge_backup.py script in misc-scripts
# Usage:
#   make -f Makefile.ai ai-smart-merge-all-backups
#
ai-smart-merge-all-backups: ai-enable-fdw ai-misc-install
	@for f in backups/*.sql; do \
		echo "Merging $$f ..."; \
		docker compose exec misc-scripts python3 /code/smart_merge_backup.py /code/$$f || exit 1; \
	done

ai-bug-report:
	curl -s -X POST http://localhost:$(PORT)/bug-report \
	  -H 'Content-Type: application/json' \
	  -d '{"description": "$(DESCRIPTION)", "reporter": "$(REPORTER)", "page": "$(PAGE)"}'

ai-bug-report-test:
	$(MAKE) -f Makefile.ai ai-bug-report \
	  DESCRIPTION='Test bug report from Makefile' \
	  REPORTER='cli-test' \
	  PAGE='/admin'

ai-suggest-enhancement:
	curl -s -X POST http://localhost:$(PORT)/suggest-enhancement \
	  -H 'Content-Type: application/json' \
	  -d '{"description": "$(DESCRIPTION)", "suggested_by": "$(SUGGESTED_BY)", "page": "$(PAGE)", "tags": [$(TAGS)], "categories": [$(CATEGORIES)]}'

ai-suggest-enhancement-test:
	$(MAKE) -f Makefile.ai ai-suggest-enhancement \
	  DESCRIPTION='Test enhancement from Makefile' \
	  SUGGESTED_BY='cli-test' \
	  PAGE='/admin' \
	  TAGS='"cli","test"' \
	  CATEGORIES='"usability"'

ai-enhancement-to-proposal:
	curl -s -X POST http://localhost:$(PORT)/enhancement-to-proposal/$(ENHANCEMENT_ID)

ai-proposal-to-enhancement:
	curl -s -X POST http://localhost:$(PORT)/proposal-to-enhancement/$(PROPOSAL_ID)

ai-accept-enhancement:
	curl -s -X POST http://localhost:$(PORT)/accept-enhancement/$(ENHANCEMENT_ID)

ai-complete-enhancement:
	curl -s -X POST http://localhost:$(PORT)/complete-enhancement/$(ENHANCEMENT_ID)

# --- Enhancement Listing Targets ---

# API host for enhancement listing (override with API_HOST=... if needed)
API_HOST ?= localhost

# List all enhancements via the API (pretty-printed with jq)
# Usage: make -f Makefile.ai ai-list-enhancements [API_HOST=host.docker.internal]
ai-list-enhancements:
	curl http://$(API_HOST):9103/enhancements | jq

ai-onboarding-health:
	python scripts/onboarding_health_check.py

# Migrate rules.db (SQLite) to Postgres using pgloader in Docker
# Usage: make -f Makefile.ai ai-migrate-sqlite-to-postgres
ai-migrate-sqlite-to-postgres:
	docker-compose up -d rules-postgres pgloader
	docker cp rules.db pgloader:/data/rules.db
	docker cp pgloader.load pgloader:/data/pgloader.load
	docker-compose exec pgloader pgloader /data/pgloader.load
	@echo "Migration from SQLite to Postgres complete!"

# Build the custom pgloader image for multi-arch (Apple Silicon) compatibility
# Usage: make -f Makefile.ai ai-build-pgloader
ai-build-pgloader:
	docker build -f Dockerfile.pgloader -t local/pgloader:latest .

# Migrate all tables from rules.db (SQLite) to Postgres using CSV and psql in Docker
# Usage: make -f Makefile.ai ai-migrate-sqlite-to-postgres-csv
ai-migrate-sqlite-to-postgres-csv:
	mkdir -p sqlite_export
	docker run --rm -v $(PWD):/data alpine:latest sh -c 'apk add --no-cache sqlite sqlite-libs > /dev/null && sqlite3 /data/rules.db ".tables" | tr " " "\n" | grep -v "^$$" | while read tbl; do sqlite3 /data/rules.db ".headers on" ".mode csv" ".output /data/sqlite_export/$$tbl.csv" "select * from \"$$tbl\";" ".output stdout"; done'
	docker run --rm -v $(PWD):/data alpine:latest sh -c 'apk add --no-cache sqlite sqlite-libs > /dev/null && sqlite3 /data/rules.db ".schema" > /data/sqlite_export/schema.sql'
	# Patch schema.sql: replace DATETIME with TIMESTAMPTZ and add DROP TABLE IF EXISTS
	sed -i '' 's/DATETIME/TIMESTAMPTZ/g' sqlite_export/schema.sql
	awk '/^CREATE TABLE /{print "DROP TABLE IF EXISTS " $$3 " CASCADE;"} 1' sqlite_export/schema.sql > sqlite_export/schema.patched.sql
	chmod +x import_csvs.sh
	docker run --rm -e PGPASSWORD=postgres -v $(PWD):/data --network ai-ide-api_default postgres:15 bash /data/import_csvs.sh
	@echo "Migration from SQLite to Postgres via CSV complete!"

# Propose a portable rule to the API
# Usage:
# make -f Makefile.ai ai-propose-portable-rule \
#   RULE_TYPE=formatting \
#   DESCRIPTION='All .mdc files must have frontmatter' \
#   DIFF='---\ndescription: ...\nglobs: ...\n---' \
#   SUBMITTED_BY=portable-rules-bot \
#   CATEGORIES='"formatting","cursor","portable"' \
#   TAGS='"formatting","cursor","portable"' \
#   PROJECT=my-shared-rules \
#   [API_PORT=9103]
ai-propose-portable-rule:
	curl -s -X POST http://localhost:$(API_PORT)/propose-rule-change \
	  -H 'Content-Type: application/json' \
	  -d '{ \
	    "rule_type": "$(RULE_TYPE)", \
	    "description": "$(DESCRIPTION)", \
	    "diff": "$(DIFF)", \
	    "submitted_by": "$(SUBMITTED_BY)", \
	    "categories": [$(CATEGORIES)], \
	    "tags": [$(TAGS)], \
	    "project": "$(PROJECT)", \
	    "reason_for_change": "$(REASON_FOR_CHANGE)", \
	    "references": "$(REFERENCES)", \
	    "current_rule": "$(CURRENT_RULE)" \
	  }'

# Rollback any failed transaction in the Postgres database (useful for clearing migration errors)
# Usage: make -f Makefile.ai ai-db-rollback
ai-db-rollback:
	docker compose exec -T db-test psql -U postgres -d rulesdb -c 'ROLLBACK;'

# Reset the database migration state (for development):
# 1. Stops all containers
# 2. Restarts only the db-test container
# 3. Runs ai-db-rollback to clear failed transactions
# 4. Restarts all containers
# 5. Runs ai-db-migrate to apply migrations
# Usage: make -f Makefile.ai ai-db-reset-migrations
ai-db-reset-migrations:
	docker compose down
	docker compose up -d db-test
	$(MAKE) -f Makefile.ai ai-db-rollback
	docker compose up -d
	$(MAKE) -f Makefile.ai ai-db-migrate

# Backup the rulesdb database to backups/rulesdb-YYYYMMDD-HHMMSS.sql
# Usage: make -f Makefile.ai ai-db-backup
ai-db-backup:
	mkdir -p backups
	docker compose exec -T db-test pg_dump -U postgres -d rulesdb > backups/rulesdb-`date +"%Y%m%d-%H%M%S"`.sql

# Drop and recreate the rulesdb database, then run migrations (development only!)
# Usage: make -f Makefile.ai ai-db-drop-recreate
ai-db-drop-recreate:
	docker compose exec -T db-test psql -U postgres -c 'DROP DATABASE IF EXISTS rulesdb;'
	docker compose exec -T db-test psql -U postgres -c 'CREATE DATABASE rulesdb;'
	$(MAKE) -f Makefile.ai ai-db-migrate

# Drop the statusenum type from Postgres (if it exists)
# Usage: make -f Makefile.ai ai-db-drop-statusenum
ai-db-drop-statusenum:
	docker compose cp scripts/drop_statusenum.sql db-test:/tmp/drop_statusenum.sql
	docker compose exec -T db-test psql -U postgres -d rulesdb -f /tmp/drop_statusenum.sql

# Danger: This will delete ALL Postgres data and volumes!
# Usage: make -f Makefile.ai ai-db-nuke
ai-db-nuke:
	docker compose down -v
	docker compose up -d db-test

# Restore the rulesdb database from a SQL backup file
# Usage: make -f Makefile.ai ai-db-restore BACKUP=backups/rulesdb-YYYYMMDD-HHMMSS.sql
ai-db-restore:
	docker compose exec -T db-test psql -U postgres -d rulesdb < $(BACKUP)

# Restore only the data (not schema) from a SQL backup file
# Usage: make -f Makefile.ai ai-db-restore-data BACKUP=backups/rulesdb-YYYYMMDD-HHMMSS.sql
ai-db-restore-data:
	cat $(BACKUP) | grep -vE '^CREATE |^ALTER |^DROP |^--' | grep -v 'COPY alembic_version' | docker compose exec -T db-test psql -U postgres -d rulesdb

# Backup only the data (no schema) from the rulesdb database
# Usage: make -f Makefile.ai ai-db-backup-data-only
ai-db-backup-data-only:
	mkdir -p backups
	docker compose exec -T db-test pg_dump --data-only -U postgres -d rulesdb > backups/rulesdb-data-`date +"%Y%m%d-%H%M%S"`.sql

# Merge Alembic heads automatically (use with caution!)
ai-db-merge-heads:
	docker compose exec api alembic heads

ai-db-heads:
	docker compose exec api alembic heads

ai-db-history:
	docker compose exec api alembic history --verbose

ai-api-wait:
	@echo "Waiting for API to be ready..."
	@until curl -sf http://localhost:$(API_PORT)/env > /dev/null; do sleep 2; done
	@echo "API is up!"

ai-db-nuke-and-restore-data:
	$(MAKE) -f Makefile.ai ai-db-backup-data-only
	$(MAKE) -f Makefile.ai ai-db-nuke
	$(MAKE) -f Makefile.ai ai-up
	$(MAKE) -f Makefile.ai ai-api-wait
	$(MAKE) -f Makefile.ai ai-db-migrate
	BACKUP=$$(ls -t backups/rulesdb-data-*.sql | head -1) $(MAKE) -f Makefile.ai ai-db-restore-data

ai-misc-install:
	docker compose exec misc-scripts pip install -r requirements.txt

ai-misc-up:
	docker compose up -d misc-scripts

ai-misc-update-rules:
	$(MAKE) -f Makefile.ai ai-misc-up
	docker compose exec misc-scripts pip install -r requirements.txt
	docker compose exec misc-scripts python update_rules.py

ai-misc-generate-update-template:
	docker compose exec misc-scripts python generate_rules_update_template.py

ai-api-restart-wait:
	docker compose restart api
	$(MAKE) -f Makefile.ai ai-api-wait

ai-propose-rules:
	bash misc_scripts/propose_rules.sh

ai-misc-mdc-to-json: ai-misc-install
	docker compose exec misc-scripts python mdc_to_json_rule_proposals.py

ai-misc-enrich-rules:
	docker compose exec misc-scripts python enrich_rule_proposals.py

ai-list-bug-reports:
	curl -s http://localhost:$(PORT)/bug-reports | jq '.'

ai-db-stamp-head:
	docker compose exec api alembic stamp head

schema-proposals:
	docker compose exec db-test psql -U postgres -d rulesdb -c '\d+ proposals' | cat

docker-pre-commit:
	docker run --rm -v $(PWD):/code -w /code local/pre-commit:latest run --all-files

docker-build-pre-commit:
	docker build -f Dockerfile.pre-commit -t local/pre-commit:latest .

ai-review-multiple-files:
	python scripts/review_multiple_files.py $(FILES)

ai-suggest-llm-rules:
	OLLAMA_URL=$(OLLAMA_URL) RULE_API_URL=$(RULE_API_URL) \
	python scripts/suggest_and_propose_rules.py $${TARGET-.}

# Run the LLM-powered rule suggestion pipeline inside Docker
# Usage: make -f Makefile.ai ai-suggest-llm-rules-docker TARGET=your/dir/or/file
ai-suggest-llm-rules-docker:
	docker compose run --rm \
	  -e RUNNING_IN_DOCKER=1 \
	  -e OLLAMA_URL=$(OLLAMA_URL) -e RULE_API_URL=$(RULE_API_URL) \
	  ollama-functions python scripts/suggest_and_propose_rules.py $${TARGET-.}

# Build and run the LLM rule suggester FastAPI service
# Usage:
#   make -f Makefile.ai ai-build-ollama-functions-service
#   make -f Makefile.ai ai-run-ollama-functions-service
OLLAMA_FUNCTIONS_IMAGE ?= ollama-functions:latest

# Default Ollama URL for LLM rule suggester service (can be overridden)
OLLAMA_URL ?= http://host.docker.internal:11434/api/generate

ai-build-ollama-functions-service:
	docker build -f Dockerfile.ollama_functions -t $(OLLAMA_FUNCTIONS_IMAGE) .

ai-run-ollama-functions-service: ai-build-ollama-functions-service
	docker run --rm -e RUNNING_IN_DOCKER=1 \
	  --add-host=host.docker.internal:host-gateway \
	  -e OLLAMA_URL=$(OLLAMA_URL) \
	  -e OLLAMA_MODEL=$(OLLAMA_MODEL) \
	  -v $(PWD):/code -w /code \
	  -p 8000:8000 \
	  $(OLLAMA_FUNCTIONS_IMAGE)

# Find the Docker bridge gateway IP
ai-docker-gateway-ip:
	@docker network inspect bridge | grep Gateway | head -1 | awk -F '"' '{print $$4}'

# Start Ollama on the Docker gateway IP
# Usage: make -f Makefile.ai ai-ollama-serve-docker-gateway
# (You may want to run this in a separate terminal)
ai-ollama-serve-docker-gateway:
	@echo "Starting Ollama with OLLAMA_HOST=0.0.0.0..."
	OLLAMA_HOST=0.0.0.0 ollama serve

# Note: If you use the gateway IP, set OLLAMA_URL to http://<gateway-ip>:11434/api/generate for ai-run-ollama-functions-service

# Kill all running Ollama processes
ai-ollama-kill:
	pkill -f 'ollama serve' || true

# Restart Ollama on the Docker gateway IP
# Usage: make -f Makefile.ai ai-ollama-restart-docker-gateway
ai-ollama-restart-docker-gateway: ai-ollama-kill
	$(MAKE) -f Makefile.ai ai-ollama-serve-docker-gateway

# Note: The following targets run Ollama in the foreground. For automation, run in the background:
#   make -f Makefile.ai ai-ollama-serve-docker-gateway &
# or
#   nohup make -f Makefile.ai ai-ollama-serve-docker-gateway > ollama.log 2>&1 &
# Or use the provided background target:
#   make -f Makefile.ai ai-ollama-serve-docker-gateway-bg

ai-ollama-serve-docker-gateway-bg:
	nohup $(MAKE) -f Makefile.ai ai-ollama-serve-docker-gateway > ollama.log 2>&1 &

# View the last 100 lines of the Ollama log file from the background run
# Usage: make -f Makefile.ai ai-ollama-logs
ai-ollama-logs:
	tail -n 100 ollama.log

ai-admin-frontend-nocache-restart:
	docker compose build --no-cache frontend
	docker compose restart frontend

# Download a model for Ollama (default: $(OLLAMA_MODEL))
# Usage: make -f Makefile.ai ai-ollama-pull-model [OLLAMA_MODEL=modelname]
ai-ollama-pull-model:
	ollama pull $(OLLAMA_MODEL)

# Onboarding: One-command setup for LLM/Ollama integration
ai-llm-setup:
	@echo "[LLM Setup] Downloading Ollama model if not present..."
	$(MAKE) -f Makefile.ai ai-ollama-pull-model
	@echo "[LLM Setup] Starting Ollama service (docker gateway)..."
	$(MAKE) -f Makefile.ai ai-ollama-serve-docker-gateway &
	@echo "[LLM Setup] Waiting for Ollama service to be ready..."
	@sleep 5
	@echo "[LLM Setup] Verifying Ollama service is running..."
	@if curl -sf http://localhost:11434 | grep -q 'Ollama'; then \
		echo '[LLM Setup] Ollama service is running!'; \
		echo '[LLM Setup] You can now use LLM-powered features.'; \
	else \
		echo '[LLM Setup] ERROR: Ollama service did not start. Please check logs with: make -f Makefile.ai ai-ollama-logs'; \
		exit 1; \
	fi

# --- Dockerized Code Review & Linting Targets ---
# Usage:
#   make -f Makefile.ai ai-lint-rule-docker [RULE_FILE=/app/.cursor/rules/yourfile.mdc]
#   make -f Makefile.ai ai-auto-feedback-docker [RULE_FILE=/app/.cursor/rules/yourfile.mdc]
#   make -f Makefile.ai ai-batch-suggest-rules-docker
#
# These run the scripts inside the misc-scripts Docker Compose service for reproducibility.

RULE_FILE ?= /app/.cursor/rules/ai_augmented_code_review_workflow.mdc

ai-lint-rule-docker:
	docker compose exec misc-scripts python /scripts/lint_rule.py $(RULE_FILE)

ai-auto-feedback-docker:
	docker compose exec misc-scripts python /scripts/auto_feedback.py $(RULE_FILE)

ai-batch-suggest-rules-docker:
	docker compose exec misc-scripts python /scripts/batch_suggest_rules.py

# Lint all .mdc rule files in Docker Compose using the new YAML frontmatter linter
# Usage:
#   make -f Makefile.ai ai-lint-mdc-docker [MDC_PATH=/app/.cursor/rules/]
#
# By default, lints all .mdc files in /app/.cursor/rules/
MDC_PATH ?= /app/.cursor/rules/

ai-lint-mdc-docker:
	docker compose exec misc-scripts python /scripts/lint_mdc.py $(MDC_PATH)

# Backup both schema+data and data-only for maximum flexibility
ai-db-backup-all:
	docker compose exec db-test pg_dump -U postgres -d rulesdb > backups/rulesdb-$(shell date +%Y%m%d-%H%M%S).sql
	docker compose exec db-test pg_dump -U postgres -d memorydb > backups/memorydb-$(shell date +%Y%m%d-%H%M%S).sql

schema-table:
	docker compose exec db-test psql -U postgres -d rulesdb -c '\d+ $(TABLE)' | cat

db-query:
	docker compose exec db-test psql -U postgres -d rulesdb -c "$(QUERY)" | cat

ai-misc-build:
	docker compose build misc-scripts

ai-misc-update-missing-user-stories:
	docker compose exec misc-scripts python /scripts/update_missing_user_stories.py

# Build the ollama-functions service
ai-build-ollama-functions:
	docker-compose build ollama-functions

# Start the ollama-functions service
ai-up-ollama-functions:
	docker-compose up -d ollama-functions

# Rebuild and restart the ollama-functions service
ai-restart-ollama-functions:
	docker-compose build ollama-functions
	docker-compose up -d ollama-functions

# Health check for ollama-functions
ai-ollama-functions-health:
	curl -s http://localhost:9104/healthz || echo '{"error": "ollama-functions not running"}'

# View the last 100 lines of logs for the ollama-functions container
ai-ollama-functions-logs:
	docker-compose logs --tail=100 ollama-functions

# Create a new Alembic migration with autogeneration
# Usage: make -f Makefile.ai ai-db-migration-new NAME=add_some_feature
ai-db-migration-new:
	docker compose exec api alembic revision --autogenerate -m "$(NAME)"

# Restart the frontend container without rebuilding
ai-admin-frontend-restart:
	docker compose restart frontend

# Show a summary of commits since a given time (default: 24 hours ago)
git-log-recent:
	@if [ -z "$(SINCE)" ]; then \
		SINCE='24 hours ago'; \
	else \
		SINCE='$(SINCE)'; \
	fi; \
	git log --since="$$SINCE" --stat --oneline | cat

ai-onboarding-help:
	@echo "\n==== AI-IDE Onboarding Quick Start ===="
	@echo "1. Fetch the latest OpenAPI docs (API reference):"
	@echo "   make -f Makefile.ai ai-docs"
	@echo "   (or visit http://localhost:9103/docs in your browser)"
	@echo "2. Start all services if needed:"
	@echo "   make -f Makefile.ai ai-up"
	@echo "3. For more onboarding info, see ONBOARDING_OTHER_AI_IDE.md"
	@echo "4. All automation (tests, builds, migrations) MUST run through Makefile.ai targets."
	@echo "5. Use Docker service names and internal ports for all connections."
	@echo "\n==== End Onboarding Quick Start ===="

ai-docker-ps:
	docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Ports}}"

ai-port-3000-procs:
	@echo "Processes using port 3000:"
	lsof -i :3000 || echo "No process found on port 3000."

ai-docker-stop-all:
	@echo "Stopping all running Docker containers..."
	docker stop $(shell docker ps -q) || echo "No running containers to stop."

ai-kill-port-3000:
	@echo "Killing any process using port 3000..."
	PID=$$(lsof -ti :3000); if [ -n "$$PID" ]; then kill -9 $$PID && echo "Killed process $$PID on port 3000."; else echo "No process found on port 3000."; fi

ai-frontend-volumes:
	@echo "Checking for volume mounts in docker-compose.yml (frontend service):"
	@awk '/frontend:/, /^[^ ]/' docker-compose.yml | grep -A 5 'volumes:' || echo "No volumes found for frontend."

ai-playwright-test:
	docker compose build frontend
	docker compose run --rm playwright

ai-playwright-build-nocache:
	docker compose build --no-cache playwright

# Start only the db-test (database) service
ai-db-up:
	docker compose up -d db-test

# --- MemoryDB backup/restore targets ---

# Backup memorydb schema and data
ai-memorydb-backup:
	mkdir -p backups
	docker compose exec -T db-test pg_dump -U postgres -d memorydb > backups/memorydb-`date +"%Y%m%d-%H%M%S"`.sql

# Restore memorydb from a backup file
# Usage: make -f Makefile.ai ai-memorydb-restore BACKUP_FILE=backups/memorydb-latest.sql
ai-memorydb-restore:
	docker compose exec -T db-test psql -U postgres -d memorydb < $(BACKUP_FILE)

# Schema-only backup for memorydb
ai-memorydb-schema-backup:
	mkdir -p backups
	docker compose exec -T db-test pg_dump -U postgres -d memorydb --schema-only > backups/memorydb-schema-`date +"%Y%m%d-%H%M%S"`.sql

# Backup only the data (no schema) from the memorydb database
ai-memorydb-backup-data-only:
	mkdir -p backups
	docker compose exec -T db-test pg_dump --data-only -U postgres -d memorydb > backups/memorydb-data-`date +"%Y%m%d-%H%M%S"`.sql

# Restore only the data (not schema) from a SQL backup file to memorydb
# Usage: make -f Makefile.ai ai-memorydb-restore-data BACKUP_FILE=backups/memorydb-data-YYYYMMDD-HHMMSS.sql
ai-memorydb-restore-data:
	docker compose exec -T db-test psql -U postgres -d memorydb < $(BACKUP_FILE)

# --- End of MemoryDB backup/restore targets ---

# --- Combined all-db backup/restore targets ---

# Restore both (edit to use correct backup files)
ai-db-restore-all:
	docker compose exec -T db-test psql -U postgres -d rulesdb < $(RULESDB_FILE)
	docker compose exec -T db-test psql -U postgres -d memorydb < $(MEMORYDB_FILE)

# --- Memory Graph Utility Targets ---

ai-memory-utils-build:
	docker build -f Dockerfile.memory-utils -t memory-utils .

ai-memory-add-node:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_add_node.sh $(NAMESPACE) $(CONTENT) $(EMBEDDING) $(META)

ai-memory-add-edge:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_add_edge.sh $(FROM_ID) $(TO_ID) $(REL_TYPE) $(META)

ai-memory-list-nodes:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_list_nodes.sh

ai-memory-list-edges:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_list_edges.sh

ai-memory-traverse-single-hop:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_traverse_single_hop.sh $(NODE_ID)

ai-memory-traverse-multi-hop:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_traverse_multi_hop.sh $(NODE_ID)

ai-memory-traverse-by-relation:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_traverse_by_relation.sh $(NODE_ID) $(REL_TYPE)

ai-memory-export-dot:
	docker run --rm --network=host -v $(PWD)/scripts:/scripts memory-utils /scripts/memory_export_dot.sh > graph.dot

# Usage:
# make -f Makefile.ai ai-memory-add-node NAMESPACE=notes CONTENT="My note" EMBEDDING="[0.1,0.2,...]" META='{"tags":["example"]}'
# make -f Makefile.ai ai-memory-add-edge FROM_ID=... TO_ID=... REL_TYPE=related_to META='{"note":"Example edge"}'
# make -f Makefile.ai ai-memory-list-nodes
# make -f Makefile.ai ai-memory-list-edges
# make -f Makefile.ai ai-memory-traverse-single-hop NODE_ID=...
# make -f Makefile.ai ai-memory-traverse-multi-hop NODE_ID=...
# make -f Makefile.ai ai-memory-traverse-by-relation NODE_ID=... REL_TYPE=...
# make -f Makefile.ai ai-memory-export-dot

ai-memory-delete-nodes:
	@if [ -z "$(NAMESPACE)" ]; then \
	  echo 'Deleting ALL memory nodes!'; \
	  curl -X DELETE http://localhost:9103/memory/nodes; \
	else \
	  echo 'Deleting memory nodes in namespace: $(NAMESPACE)'; \
	  curl -X DELETE "http://localhost:9103/memory/nodes?namespace=$(NAMESPACE)"; \
	fi
# Usage:
# make -f Makefile.ai ai-memory-delete-nodes NAMESPACE=testns
# make -f Makefile.ai ai-memory-delete-nodes   # (deletes ALL nodes)

# Note: Alembic migrations are executed in the API container (not the test container).
# Run the backup/restore end-to-end test (destructive!)
ai-test-backup-restore:
	$(MAKE) -f Makefile.ai ai-up
	$(MAKE) -f Makefile.ai ai-api-wait
	docker-compose run --rm -e RUN_BACKUP_RESTORE_TEST=1 -e API_URL=http://api:8000 test pytest tests/test_backup_restore.py -x -s
# Usage:
# make -f Makefile.ai ai-test-backup-restore

ai-test-unit:
	docker-compose run --rm test pytest tests/unit/ --disable-warnings --tb=short

ai-scan-memory-nodes:
	docker compose run --rm ollama-functions python /code/scripts/scan_for_memory_node_opportunities.py

ai-stop-ollama-functions:
	docker compose stop ollama-functions

ai-down-ollama-functions:
	docker compose rm -sf ollama-functions

ai-test-coverage:
	docker-compose run --rm test pytest --cov=. --cov-report=term-missing --cov-report=html tests/

ai-test-integration:
	docker-compose run --rm test pytest tests/integration $(PYTEST_ARGS)

ai-memorydb-migrate:
	docker compose exec api alembic -c alembic_memorydb.ini upgrade head

# Search memory nodes by text (preferred)
ai-memory-search-text:
	curl -X POST http://localhost:9103/memory/nodes/search \
	  -H "Content-Type: application/json" \
	  -d '{"text": "$(TEXT)", "namespace": "$(NAMESPACE)", "limit": $(LIMIT)}'

# Search memory nodes by embedding (advanced)
ai-memory-search-embedding:
	curl -X POST http://localhost:9103/memory/nodes/search \
	  -H "Content-Type: application/json" \
	  -d '{"embedding": $(EMBEDDING), "namespace": "$(NAMESPACE)", "limit": $(LIMIT)}'

# Usage:
# make -f Makefile.ai ai-memory-search-text TEXT="Find similar ideas about AI memory." NAMESPACE=notes LIMIT=5
# make -f Makefile.ai ai-memory-search-embedding EMBEDDING="[0.1,0.2,...]" NAMESPACE=notes LIMIT=5

#
# ai-search-user-stories: Search user stories in docs/user_stories/ for a keyword.
# Usage:
#   make -f Makefile.ai ai-search-user-stories KEYWORD=merge
#
ai-search-user-stories:
	python3 scripts/search_user_stories.py $(KEYWORD)

#
# ai-fdw-setup-script: Run the FDW setup steps for a given temp DB and target DB (for troubleshooting and reference).
# Usage:
#   make -f Makefile.ai ai-fdw-setup-script TEMP_DB=temp_restore_db_xxxx TARGET_DB=memorydb
#
ai-fdw-setup-script:
	docker compose exec db-test psql -U postgres -d $(TARGET_DB) -c "CREATE EXTENSION IF NOT EXISTS postgres_fdw;"
	docker compose exec db-test psql -U postgres -d $(TARGET_DB) -c "DROP SERVER IF EXISTS temp_restore_server CASCADE;"
	docker compose exec db-test psql -U postgres -d $(TARGET_DB) -c "CREATE SERVER temp_restore_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host='db-test', dbname='$(TEMP_DB)', port='5432');"
	docker compose exec db-test psql -U postgres -d $(TARGET_DB) -c "CREATE USER MAPPING FOR CURRENT_USER SERVER temp_restore_server OPTIONS (user='postgres');"
	docker compose exec db-test psql -U postgres -d $(TARGET_DB) -c "IMPORT FOREIGN SCHEMA public FROM SERVER temp_restore_server INTO temp_schema;"